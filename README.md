# AlphaZero for Connect4 ä»é›¶å¼€å§‹å­¦ä¹ ä¸‹å››å­æ£‹ ğŸ¤”

**Author:** Zhihan Yang @ Carleton College (MN, USA)

**Keywords:** board game, MCTS, deep convolutional neural network, self-play, PyTorch

This is a learning resource, and likely will not be tractable for games using bigger boards. 

Feel free to ask questions through Github Issues.

**ä½œè€…ï¼š** æ¨ä¹‹æ¶µ @ å¡å°”é¡¿å­¦é™¢

**å…³é”®è¯ï¼š** æ£‹ç±»æ¸¸æˆï¼Œè’™ç‰¹å¡æ´›æ ‘æœï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼Œè‡ªæˆ‘å¯¹å¼ˆï¼ŒPyTorch

è¿™ä¸ªä»£ç åº“å¯ä»¥å¸®åŠ©å¤§å®¶äº†è§£AlphaZeroï¼Œä½†æ˜¯å¯¹äºæ£‹ç›˜æ›´å¤§çš„æ¸¸æˆä¼°è®¡å®Œå…¨è·‘ä¸åŠ¨ã€‚

æ¬¢è¿é€šè¿‡Github Issuesæé—®ã€‚

## Why Connect4 ä¸ºä»€ä¹ˆé€‰æ‹©å››å­æ£‹

Connect4 is a middle ground between Connect3 (tic-tac-toe) and Connect5 (Gomoku or Gobang). It is much more difficult than Connect3, but it is also much easier than Connect5. Also, in Connect4, if the first-hand player plays optimally, it will win for sure; this makes it easier to verify how well AlphaZero learned. Here, we use a 6x6 board for Connect4.

å››å­æ£‹æ˜¯ä¸‰å­æ£‹ï¼ˆtic-tac-toeï¼‰å’Œäº”å­æ£‹çš„è¿‡æ¸¡ã€‚ä¹‹æ‰€ä»¥é€‰æ‹©å››å­æ£‹ï¼Œæ˜¯å› ä¸ºå››å­æ£‹æ¯”ä¸‰å­æ£‹éš¾å¾ˆå¤šï¼Œä½†åˆæ¯”äº”å­æ£‹ç®€å•å¾ˆå¤šã€‚æ­¤å¤–ï¼Œåœ¨å››å­æ£‹ä¸­ï¼Œä¸€ä¸ªå®Œç¾ç©å®¶åœ¨å…ˆæ‰‹çš„æƒ…å†µä¸‹å¯ä»¥ç™¾åˆ†ç™¾è·èƒœï¼Œæ–¹ä¾¿æˆ‘ä»¬éªŒè¯AlphaZeroå­¦ä¹ çš„ç»“æœã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨6x6çš„æ£‹ç›˜ã€‚

## Example game plays vs human äººæœºå¯¹å¼ˆç»“æœ

Before we talk about theory and code, let's see what AlphaZero can do after 3000 self-play games. 

During training, AlphaZero uses 500 MCTS simulations for each move; during evaluation (for the games below), AlphaZero uses 50-1000 MCTS iterations (randomly picked between this range) to induce some stochasticity. 

In all games below, AlphaZero is the first-hand player and holds the black stone, and the values in the background show the prior move probabilities predicted by the convolutional neural network. The human player (me) is the second-hand player and holds the white stone. The final winning move AlphaZero is shadowed.

åœ¨è®¨è®ºç†è®ºå’Œä»£ç ä¹‹å‰ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹AlphaZeroåœ¨3000å±€self-playä¹‹åèƒ½è¾¾åˆ°ä»€ä¹ˆæ ·çš„æ•ˆæœã€‚

åœ¨è®­ç»ƒå½“ä¸­ï¼ŒAlphaZeroæ¯ä¸€æ­¥ä½¿ç”¨500æ¬¡MCTS simuationï¼›åœ¨æµ‹è¯•ä¸­ï¼ˆä»¥ä¸‹å¯¹å¼ˆï¼‰ï¼ŒAlphaZeroæ¯ä¸€æ­¥ä½¿ç”¨50åˆ°1000æ¬¡ï¼ˆ`np.random.randint`ï¼‰MCTS simuationæ¥äº§ç”Ÿä¸€å®šçš„éšæœºæ€§ã€‚

åœ¨ä»¥ä¸‹å¯¹å¼ˆä¸­ï¼ŒAlphaZeroæ˜¯å…ˆæ‰‹ç©å®¶å¹¶æŒæœ‰é»‘æ£‹ã€‚æ£‹ç›˜ä¸Šæ˜¾ç¤ºçš„æ•°å­—æ˜¯å·ç§¯ç¥ç»ç½‘ç»œé¢„æµ‹å‡ºçš„prior move probabilitiesã€‚äººç±»ç©å®¶ï¼ˆæˆ‘ï¼‰æ˜¯åæ‰‹å¹¶æŒæœ‰ç™½æ£‹ã€‚AlphaZeroæœ€åçš„æ”¾æ£‹ä½ç½®ç”¨æ·±è‰²æ ‡å‡ºã€‚

Game 1:

![Image](readme_images/game1.png?raw=true)

Game 2:

![Image](readme_images/game2.png?raw=true)

Game 3:

![Image](readme_images/game3.png?raw=true)

## Theory tutorial ç†è®ºæ•™ç¨‹

A detailed tutorial is available in this repo; see `alphazero.pdf`.

è¿™ä¸ªä»£ç åº“åŒ…å«ä¸€ä»½è¯¦ç»†çš„ç†è®ºæ•™ç¨‹ï¼Œè¯·è§`alphazero.pdf`.

## Code tutorial ä»£ç æ•™ç¨‹

Please first make sure that your working directory is `alpha_zero` by `cd alpha_zero`.

How to play against pure MCTS (early moves can take up to 25 seconds but later moves are quicker): 

`python connect4_mcts_vs_human.py`

How to play against pre-trained AlphaZero (moves are fast): 

`python connect4_alphazero_vs_human.py`

How to train AlphaZero from scratch (3000 self-play games & supervised learning takes around 7-8 hours on GPU): 

`python connect4_train_alphazero.py`

Within the training script, you will see `wandb.init`. `wandb` stands for Weights and Biases, a website for tracking machine learning training runs. You can learn about it from their website or their YouTube channel, and change my username and run name to yours. Here, during training, the parameters of the convolutional policy-value network is saved locally as a `pth` file, but they are all uploaded to the cloud (i.e., to `wandb`). This was convenient for me because I didn't know how to download a file from a remote machine. 

After training has finished, you can put `pvnet_3000.pth` in `trained_models` (in `alpha_zero`) and `connect4_alphazero_vs_human.py` will automatically use it.

## Potential improvements å¯ä»¥æå‡çš„åœ°æ–¹

AlphaZero involves both MCTS and deep learning. Python isn't the best language for implementing MCTS because it is slow. However, Python is one of the best languages for doing deep learning. These two factors together form a dilemma because I want to use C++ for MCTS but I don't know how to do supervised learning using C++.

In short, I'm much more familiar with Python than C++.

AlphaZeroåŒæ—¶åŒ…å«MCTSå’Œæ·±åº¦å­¦ä¹ çš„å…ƒç´ ã€‚Pythonå¹¶ä¸æ˜¯æœ€é€‚åˆå®ç°MCTSçš„è¯­è¨€ï¼Œå› ä¸ºå®ƒå¤ªæ…¢äº†ã€‚ä½†Pythonç¡®å®æ˜¯åšdeep learningæœ€å¥½çš„è¯­è¨€ä¹‹ä¸€ã€‚è¿™ä¸¤ä¸ªå› ç´ åŠ åœ¨ä¸€èµ·è®©æˆ‘æœ‰äº›ä¸ºéš¾ï¼Œå› ä¸ºæˆ‘å¸Œæœ›ç”¨C++æ¥å†™MCTSï¼Œä½†æˆ‘å¹¶ä¸çŸ¥é“å¦‚ä½•åœ¨C++ä¸­åšæ·±åº¦å­¦ä¹ ã€‚

æ€»è€Œè¨€ä¹‹ï¼Œç›¸æ¯”äºC++ï¼Œæˆ‘å¯¹Pythonçš„ç†Ÿæ‚‰åº¦é«˜å¾ˆå¤šã€‚

## References å¯¹æˆ‘å¾ˆæœ‰å¸®åŠ©çš„èµ„æº


