# AlphaZero for Connect4 ä»é›¶å¼€å§‹å­¦ä¹ ä¸‹å››å­æ£‹ ğŸ¤”

**Author:** Zhihan Yang @ Carleton College (MN, USA)

**Keywords:** board game, MCTS, deep convolutional neural network, self-play, PyTorch

This is a learning resource, and likely will not be tractable for games using bigger boards. 

Feel free to ask questions through Github Issues.

**ä½œè€…ï¼š** æ¨ä¹‹æ¶µ @ å¡å°”é¡¿å­¦é™¢

**å…³é”®è¯ï¼š** æ£‹ç±»æ¸¸æˆï¼Œè’™ç‰¹å¡æ´›æ ‘æœï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼Œè‡ªæˆ‘å¯¹å¼ˆï¼ŒPyTorch

è¿™ä¸ªä»£ç åº“å¯ä»¥å¸®åŠ©å¤§å®¶äº†è§£AlphaZeroï¼Œä½†æ˜¯å¯¹äºæ£‹ç›˜æ›´å¤§çš„æ¸¸æˆä¼°è®¡å®Œå…¨è·‘ä¸åŠ¨ã€‚

æ¬¢è¿é€šè¿‡Github Issuesæé—®ã€‚

## Why Connect4 ä¸ºä»€ä¹ˆé€‰æ‹©å››å­æ£‹

Connect4 is a middle ground between Connect3 (tic-tac-toe) and Connect5 (Gomoku or Gobang). It is much more difficult than Connect3, but it is also much easier than Connect5. Also, in Connect4, if the first-hand player plays optimally, it will win for sure; this makes it easier to verify how well AlphaZero learned. Here, we use a 6x6 board for Connect4.

å››å­æ£‹æ˜¯ä¸‰å­æ£‹ï¼ˆtic-tac-toeï¼‰å’Œäº”å­æ£‹çš„è¿‡æ¸¡ã€‚ä¹‹æ‰€ä»¥é€‰æ‹©å››å­æ£‹ï¼Œæ˜¯å› ä¸ºå››å­æ£‹æ¯”ä¸‰å­æ£‹éš¾å¾ˆå¤šï¼Œä½†åˆæ¯”äº”å­æ£‹ç®€å•å¾ˆå¤šã€‚æ­¤å¤–ï¼Œåœ¨å››å­æ£‹ä¸­ï¼Œä¸€ä¸ªå®Œç¾ç©å®¶åœ¨å…ˆæ‰‹çš„æƒ…å†µä¸‹å¯ä»¥ç™¾åˆ†ç™¾è·èƒœï¼Œæ–¹ä¾¿æˆ‘ä»¬éªŒè¯AlphaZeroå­¦ä¹ çš„ç»“æœã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨6x6çš„æ£‹ç›˜ã€‚

## Example game plays vs human äººæœºå¯¹å¼ˆç»“æœ

Before we talk about theory and code, let's see what AlphaZero can do after 3000 self-play games. 

During training, AlphaZero uses 500 MCTS simulations for each move; during evaluation (for the games below), AlphaZero uses 50-1000 MCTS iterations (randomly picked between this range) to induce some stochasticity. 

In all games below, AlphaZero is the first-hand player and holds the black stone, and the values in the background show the prior move probabilities predicted by the convolutional neural network. The human player (me) is the second-hand player and holds the white stone.

The final winning move AlphaZero is shadowed.

åœ¨è®¨è®ºç†è®ºå’Œä»£ç ä¹‹å‰ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹AlphaZeroåœ¨3000å±€self-playä¹‹åèƒ½è¾¾åˆ°ä»€ä¹ˆæ ·çš„æ•ˆæœã€‚

åœ¨è®­ç»ƒå½“ä¸­ï¼ŒAlphaZeroæ¯ä¸€æ­¥ä½¿ç”¨500æ¬¡MCTS simuationï¼›åœ¨æµ‹è¯•ä¸­ï¼ˆä»¥ä¸‹å¯¹å¼ˆï¼‰ï¼ŒAlphaZeroæ¯ä¸€æ­¥ä½¿ç”¨50åˆ°1000æ¬¡ï¼ˆ`np.random.randint`ï¼‰MCTS simuationæ¥äº§ç”Ÿä¸€å®šçš„éšæœºæ€§ã€‚

åœ¨ä»¥ä¸‹å¯¹å¼ˆä¸­ï¼ŒAlphaZeroæ˜¯å…ˆæ‰‹ç©å®¶å¹¶æŒæœ‰é»‘æ£‹ã€‚æ£‹ç›˜ä¸Šæ˜¾ç¤ºçš„æ•°å­—æ˜¯å·ç§¯ç¥ç»ç½‘ç»œé¢„æµ‹å‡ºçš„prior move probabilitiesã€‚äººç±»ç©å®¶ï¼ˆæˆ‘ï¼‰æ˜¯åæ‰‹å¹¶æŒæœ‰ç™½æ£‹ã€‚

AlphaZeroæœ€åçš„æ”¾æ£‹ä½ç½®ç”¨æ·±è‰²æ ‡å‡ºã€‚

Game 1:

![Image](readme_images/game1.png?raw=true)

Game 2:

![Image](readme_images/game2.png?raw=true)

Game 3:

![Image](readme_images/game3.png?raw=true)

## Theory tutorial ç†è®ºæ•™ç¨‹

A detailed tutorial is available in this repo; see `alphazero.pdf`.

è¿™ä¸ªä»£ç åº“åŒ…å«ä¸€ä»½è¯¦ç»†çš„ç†è®ºæ•™ç¨‹ï¼Œè¯·è§`alphazero.pdf`.

## Code tutorial ä»£ç æ•™ç¨‹

## Potential improvements å¯ä»¥æå‡çš„åœ°æ–¹

## References å¯¹æˆ‘å¾ˆæœ‰å¸®åŠ©çš„èµ„æº

